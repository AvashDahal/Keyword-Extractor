!pip install nltk



import pandas as pd


df = pd.read_csv('papers.csv')


df.head(10)


df.shape


df.info()


df.isna().sum()



df.update(df[['event_type']].fillna('Unknown'))



df.isna().sum()


import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer



nltk.download('stopwords')



stop_words = set(stopwords.words("english")) 
print(stop_words)



#custom stopwords can also be created 
new_words = [
    "fig", "figure", "image", "sample", "using",
    "show", "result", "large", "also", "one", "two",
    "three", "four", "five", "seven", "eight", "nine",
    "data", "analysis", "example", "method", "table",
    "experiment", "study", "observed", "calculated",
    "reported", "value", "values", "different",
    "approach", "various", "similar", "significant",
    "obtained", "considered", "indicated", "presented",
    "described", "number", "respectively"
]
stop_words=list(stop_words.union(new_words))


from nltk.tokenize import word_tokenize
nltk.download('punkt_tab')


#Text Preprocessing

def preprocessing_text(text):
    clean_text = text.lower()  # Converting to lowercase
    clean_text = re.sub(r'<.*?>', ' ', clean_text)  # Removing tags
    clean_text = re.sub(r'[^a-zA-Z\s]', '', clean_text)    # Removing numbers
    clean_text = word_tokenize(clean_text)
    clean_text=[word for word in clean_text if word not in stop_words]
    stemming= PorterStemmer()
    clean_text=[stemming.stem(word) for word in clean_text]
    return ' '.join(clean_text)


preprocessing_text("This moving loving  is a text <h1> <p1>I need ankle brackets removed 1 2 4 5432233 </p1> </h1>")



docs= df['paper_text'].apply(lambda x:preprocessing_text(x))


docs[0]


#Count Vectorizer 


from sklearn.feature_extraction.text import CountVectorizer

# Sample text data
documents = [
    "I love programming programming programming in Python",
    "Python is great great for data science",
    "I love data science and machine learning programming"
]

# Create CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the documents
X = vectorizer.fit_transform(documents)

# Convert to array for better readability
print(X.toarray())

# Display the feature names (words)
print(vectorizer.get_feature_names_out())



'''How CountVectorizer Works
It tokenizes the text (splits it into words).
It creates a vocabulary of all unique words in the dataset.
It represents each document as a vector where each entry corresponds to the count of a specific word in that document.'''



'''
max_df=0.9: Ignores terms that appear in more than 90% of the documents, treating them as too common (like stop words).
max_features=7000: Limits the vocabulary size to the 7,000 most frequently occurring words.
ngram_range=(1,3): Extracts unigrams (single words), bigrams (two-word combinations), and trigrams (three-word combinations).
'''


cv= CountVectorizer(max_df=0.9,max_features=7000,ngram_range=(1,3))
word_count_vectors=cv.fit_transform(docs)


word_count_vectors


#TfidfTransformer
'''
CountVectorizer extracts word counts.
TfidfTransformer converts counts into TF-IDF values.
High TF-IDF score → Important word in that document.
Low TF-IDF score → Common word across documents.
'''

'''
smooth_idf=True: This smoothing prevents division by zero when a term is not present in any document.
use_idf = True: give highest value to less accuring words (keywords)
'''


from sklearn.feature_extraction.text import TfidfTransformer


tfidf_transformer= TfidfTransformer(smooth_idf=True,use_idf=True)
tfidf_transformer=tfidf_transformer.fit(word_count_vectors)


#Now to get keywords
''' Get features names
word counts for user docs 
sorting sparse matrix conditions
extracting top 10 keywords'''


feature_names=cv.get_feature_names_out()






def print_keywords(index,keywords,df):
    print("\n======Title======")
    print(df['title'][index])
    print("\n=====abstract=====")
    print(df['abstract'][index])
    print('\n=====Keywords====')
    for k in keywords:
        print(k,keywords[k])

index=1
keywords =get_keywords(index,docs)
print_keywords(index,keywords,df)




