{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5122099-e74d-42d4-8023-f8c6994dd597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\avash dahal\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\avash dahal\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\avash dahal\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\avash dahal\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\avash dahal\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\avash dahal\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d72ef254-bc19-49a8-80c5-206e8408f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9efd60ab-8b83-4b4d-a424-37c51a2d4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fee0813-5078-429b-903d-8b293e9da919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1002</td>\n",
       "      <td>1994</td>\n",
       "      <td>Using a neural net to instantiate a deformable...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1002-using-a-neural-net-to-instantiate-a-defor...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>U sing a neural net to instantiate a\\ndeformab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1003</td>\n",
       "      <td>1994</td>\n",
       "      <td>Plasticity-Mediated Competitive Learning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1003-plasticity-mediated-competitive-learning.pdf</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Plasticity-Mediated Competitive Learning\\n\\nTe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1004</td>\n",
       "      <td>1994</td>\n",
       "      <td>ICEG Morphology Classification using an Analog...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1004-iceg-morphology-classification-using-an-a...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>ICEG Morphology Classification using an\\nAnalo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1005</td>\n",
       "      <td>1994</td>\n",
       "      <td>Real-Time Control of a Tokamak Plasma Using Ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1005-real-time-control-of-a-tokamak-plasma-usi...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Real-Time Control of a Tokamak Plasma\\nUsing N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1006</td>\n",
       "      <td>1994</td>\n",
       "      <td>Pulsestream Synapses with Non-Volatile Analogu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1006-pulsestream-synapses-with-non-volatile-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Real-Time Control of a Tokamak Plasma\\nUsing N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "5  1002  1994  Using a neural net to instantiate a deformable...        NaN   \n",
       "6  1003  1994           Plasticity-Mediated Competitive Learning        NaN   \n",
       "7  1004  1994  ICEG Morphology Classification using an Analog...        NaN   \n",
       "8  1005  1994  Real-Time Control of a Tokamak Plasma Using Ne...        NaN   \n",
       "9  1006  1994  Pulsestream Synapses with Non-Volatile Analogu...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "5  1002-using-a-neural-net-to-instantiate-a-defor...  Abstract Missing   \n",
       "6  1003-plasticity-mediated-competitive-learning.pdf  Abstract Missing   \n",
       "7  1004-iceg-morphology-classification-using-an-a...  Abstract Missing   \n",
       "8  1005-real-time-control-of-a-tokamak-plasma-usi...  Abstract Missing   \n",
       "9  1006-pulsestream-synapses-with-non-volatile-an...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  \n",
       "5  U sing a neural net to instantiate a\\ndeformab...  \n",
       "6  Plasticity-Mediated Competitive Learning\\n\\nTe...  \n",
       "7  ICEG Morphology Classification using an\\nAnalo...  \n",
       "8  Real-Time Control of a Tokamak Plasma\\nUsing N...  \n",
       "9  Real-Time Control of a Tokamak Plasma\\nUsing N...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dfe5758-8c27-4ead-9819-0e726fd93a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7241, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3630970-d053-4c1e-bbd9-12ffc38653e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7241 entries, 0 to 7240\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          7241 non-null   int64 \n",
      " 1   year        7241 non-null   int64 \n",
      " 2   title       7241 non-null   object\n",
      " 3   event_type  2422 non-null   object\n",
      " 4   pdf_name    7241 non-null   object\n",
      " 5   abstract    7241 non-null   object\n",
      " 6   paper_text  7241 non-null   object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 396.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93f17010-a37c-4bf8-9c6a-9b648a900ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "year             0\n",
       "title            0\n",
       "event_type    4819\n",
       "pdf_name         0\n",
       "abstract         0\n",
       "paper_text       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ddee7dc-c39b-43a9-a58a-ab0e736c35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.update(df[['event_type']].fillna('Unknown'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8fb7bc1-b153-4ffc-a80f-994c9ecce2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "year          0\n",
       "title         0\n",
       "event_type    0\n",
       "pdf_name      0\n",
       "abstract      0\n",
       "paper_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "defd80b2-4434-4c84-bfd4-ad5e33da4559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "056ca103-05d6-411a-8bf0-1cabe167ca7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Avash\n",
      "[nltk_data]     Dahal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c16994a-81fa-48fc-8561-62ebb0df21d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yourselves', 'doesn', 'his', 'through', \"wasn't\", \"needn't\", 'she', 'once', 'had', 'out', 'was', 'of', 'as', 'again', 'ours', 'can', 'no', \"isn't\", 'shan', 'after', 'those', \"mightn't\", 'to', 'here', 'where', 'should', 'll', 'o', \"don't\", \"won't\", 'you', 'didn', 'into', 'my', 'they', 'it', 'be', 'now', 'ma', 'don', \"doesn't\", 'on', 'than', \"hadn't\", 'such', 'from', \"aren't\", 'while', 'own', 'a', 'ourselves', 'does', 'very', 'wasn', 'aren', 'down', 'up', 'over', \"haven't\", 'me', 'am', 'm', \"you're\", 'other', 'above', 'doing', 'same', 'i', 'hers', 'and', 'when', 'few', 'been', 'or', 'nor', 'about', 'herself', 'there', 'all', 'hadn', 'him', 'so', 'haven', 'mustn', 'at', 'the', 'her', 'under', 'these', 't', 'against', 'will', 'both', 'have', 'theirs', 'yours', 'wouldn', 'in', \"she's\", 'having', 'has', 'its', 'did', 'during', 're', \"wouldn't\", 'our', 'myself', 'why', \"should've\", 'then', 'is', 'who', 'mightn', 'more', \"mustn't\", 'off', \"shan't\", 'if', 'too', 's', 'ain', 'we', 'how', 'an', 'were', 'them', 'won', 'shouldn', 'below', 'for', 'between', 'each', 'but', \"hasn't\", 'needn', \"weren't\", 'that', \"it's\", 'your', 'd', 'by', \"couldn't\", 'not', 'himself', \"you'll\", 'because', 'just', 'this', 'themselves', 'are', 'only', \"didn't\", 'what', 'isn', \"shouldn't\", 'y', 'weren', 'further', 'do', 'until', 'before', 'hasn', 'any', 'yourself', 'most', 'which', \"that'll\", \"you'd\", 'whom', 'being', \"you've\", 'itself', 've', 'couldn', 'he', 'their', 'some', 'with'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) \n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddd429a3-0fba-46ca-bac1-0524add9d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom stopwords can also be created \n",
    "new_words = [\n",
    "    \"fig\", \"figure\", \"image\", \"sample\", \"using\",\n",
    "    \"show\", \"result\", \"large\", \"also\", \"one\", \"two\",\n",
    "    \"three\", \"four\", \"five\", \"seven\", \"eight\", \"nine\",\n",
    "    \"data\", \"analysis\", \"example\", \"method\", \"table\",\n",
    "    \"experiment\", \"study\", \"observed\", \"calculated\",\n",
    "    \"reported\", \"value\", \"values\", \"different\",\n",
    "    \"approach\", \"various\", \"similar\", \"significant\",\n",
    "    \"obtained\", \"considered\", \"indicated\", \"presented\",\n",
    "    \"described\", \"number\", \"respectively\"\n",
    "]\n",
    "stop_words=list(stop_words.union(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e993aef2-6ee1-420e-95ae-c4066523697c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Avash\n",
      "[nltk_data]     Dahal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adc0c9cf-d593-42d3-85fa-f6ae73a3b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    clean_text = text.lower()  # Converting to lowercase\n",
    "    clean_text = re.sub(r'<.*?>', ' ', clean_text)  # Removing tags\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', '', clean_text)    # Removing numbers\n",
    "    clean_text = word_tokenize(clean_text)\n",
    "    clean_text=[word for word in clean_text if word not in stop_words]\n",
    "    stemming= PorterStemmer()\n",
    "    clean_text=[stemming.stem(word) for word in clean_text]\n",
    "    return ' '.join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a517255-bc0c-4cac-8b80-d60a2ad97a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'move love text need ankl bracket remov'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_text(\"This moving loving  is a text <h1> <p1>I need ankle brackets removed 1 2 4 5432233 </p1> </h1>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37f9d97d-87e7-4d60-91d1-4beb5929af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= df['paper_text'].apply(lambda x:preprocessing_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57a4d64f-3b37-493d-8556-6c848d3a4bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'selforgan associ databas applic hisashi suzuki suguru arimoto osaka univers toyonaka osaka japan abstract effici selforgan associ databas propos togeth applic robot eyesight system propos databas associ input output first half part discuss algorithm selforgan propos aspect hardwar produc new style neural network latter half part applic handwritten letter recognit autonom mobil robot system demonstr introduct let map f x given x finit infinit set anoth finit infinit set learn machin observ set pair x sampl randomli x x x x mean cartesian product x comput estim j x f make small estim error measur usual say faster decreas estim error increas sampl better learn machin howev express perform incomplet sinc lack consider candid j j assum preliminarili find good learn machin clarifi concept let us discuss type learn machin let us advanc understand selforgan associ databas paramet type ordinari type learn machin assum equat relat xs ys paramet indefinit name structur f equival defin implicitli set f candid f subset map x comput paramet base sampl call type paramet type learn machin defin well f f j approach f sampl increas altern case howev estim error remain etern thu problem design learn machin return find proper structur f sens hand assum structur f demand compact possibl achiev fast learn word paramet small sinc paramet j uniqu determin even though sampl howev demand proper contradict compact consequ paramet type better compact assum structur proper better learn machin elementari concept design learn machin univers ordinari neural network suppos suffici knowledg f given though j unknown case compar easi find proper compact structur j altern case howev sometim difficult possibl solut give compact assum almighti structur cover combin orthogon base infinit dimens structur neural network approxim truncat finit dimens implement american institut physic main topic design neural network establish desir structur work includ develop practic procedur comput coeffici sampl discuss flourish sinc mani effici method propos recent even hardwar unit comput coeffici parallel speedup sold eg anza mark iii odyssey e nevertheless neural network alway exist danger error remain etern estim precis speak suppos combin base finit defin structur essenti word suppos f locat near f case estim error none neglig howev distant f estim error never becom neglig inde mani research report follow situat appear complex estim error converg sampl increas decreas hardli even though dimens heighten properti sometim consider defect neural network recursi type recurs type found anoth methodolog learn follow initi stage set fa instead notat f candid equal set map x observ first xl yl e x x fa reduc fi ixt yl e f observ second x e x x fl reduc f ixt yl ix e f thu candid set f becom gradual small observ sampl proce observ isampl write likelihood estim select fi henc contrarili paramet type recurs type guarante sure j approach sampl increas recurs type observ x yd rewrit lx ix xs correl henc type architectur compos rule rewrit free memori space architectur form natur kind databas build manag system selforgan way howev databas differ ordinari one follow sens record sampl alreadi comput estim lx x e x call databas associ databas first subject construct associ databas establish rule rewri ting purpos adap measur call dissimilari ty dissimilari ty mean map x x x real x x e x x x dx x whenev lx x howev necessarili defin singl formula defin collect rule written form dissimilar defin structur local x x henc even though knowledg f imperfect reflect heurist way henc contrarili neural network possibl acceler speed learn establish well especi easili find simpl ds ls process analog inform like human see applic paper recurs type show strongli effect denot sequenc sampl xl yd x simplest construct associ databas observ isampl follow algorithm initi stage let empti set everi let ilx x e x equal xy e sl dx x min yest dx x furthermor add x sl produc sa ie sl u x yn anoth version improv econom memori follow algorithm initi stage let compos arbitrari element x x everi let iilex x e x equal x e sil dx x min dx x iiesl furthermor iilxi yi let si sil add xi yi sil produc si ie si sil u xi yi either construct ii approach f increas howev comput time grow proport size si second subject construct associ databas address rule employ econom comput time subsequ chapter construct associ databas purpos propos manag form binari tree selforgan associ databas given sequenc xl yl x algorithm construct associ databas follow algorithm step iiniti let xroot yroot xl yd x variabl assign respect node memor furthermor let step increas put x reset pointer n root repeat follow n arriv termin node ie leaf notat nand dxt xn let n n mean descend node n n otherwis let n n dx rn step display yin relat inform next put yin back step otherwis first establish new descend node n n secondli let xn yin xn yin xn yin xt final back step loop step stop time continu suppos gate element name artifici synaps play role branch prepar obtain new style neural network gate element randomli connect algorithm letter recognit recen tli vertic slit recogn typograph english letter elast match recogn hand written discret english letter global train fuzzi logic search recogn chines charact written squar style etc publish selforgan associ databas realiz recognit handwritten continu english letter wn nov xk lat dwlo sourc document loo h window sampl nualber sampl es scanner take document letter recogn use parallelogram window least cover maxim letter process sequenc letter shift window recogn scan word slant direct place window left vicin may first black point detect window catch letter part succeed letter recognit head letter perform end posit name boundari line letter becom known henc start scan boundari repeat oper recogn accomplish recurs task thu major problem come identifi head letter window consid defin follow regard window imag xs defin x accordingli x x e x x x denot b black point left area boundari window x project b onto window x measur euclidean distanc fj black point b x closest b let dx x summat black point bs x divid bs regard coupl read posit boundari ys defin accordingli oper teach recogn interact relat window read boundari algorithm precis recal read incorrect oper teach correct read via consol moreov boundari posit incorrect teach correct posit via mous show partial document use show chang node recognit rate defin rel frequenc correct answer past trial speciiic window height dot width dot slant angular deg level tree distribut time recognit rate converg experiment recognit rate converg case rare case howev attain sinc eg c e distinguish excess lluctuat write consist x yrelat assur like node increas endlessli henc clever stop learn recognit rate attain upper limit improv recognit rate must consid spell word futur subject obstacl avoid movement system camera type autonom mobil robot flourishinglyo system made author belong categori mathemat methodolog solv usual problem obstacl avoid movement cost minim problem cost criterion establish artifici contrarili selforgan associ databas reproduc faith cost criterion oper therefor motion robot learn becom natur length width height robot om weight kg visual angl camera deg robot follow factor motion turn less deg advanc less control speed less kmh done passageway wid th insid build author laboratori exist experiment intent arrang box smoke stand ga cylind stool handcart etc passag way random let robot take camera recal trace rout preliminarili record purpos defin follow let camera face deg downward take process low pass filter scan vertic filter bottom top search first point c lumin chang excess su bstitu te point bottom c white point c top black obstacl exist front robot white area show free area robot move around regard binari x dot imag process thu xs defin x accordingli everi x x e x x x let dx x black point exclusiveor x x regard ys imag draw rout imag xs defin accordingli robot superimpos current camera x rout recal x inquir oper instruct oper judg subject whether suggest rout appropri neg answer draw desir rout x mous teach new robot oper defin implicitli sequenc x reflect cost criterion oper l iibub roan stationari uni configur autonom mobil robot system north rmbi ie unit robot roan experiment environ wall camera preprocess fa preprocess cours suggest ion search process obstacl avoid movement x process posit identif defin satisfact rate rel frequenc accept suggest rout past trial typic chang satisfact rate show tendenc attain around time notic rest mean directli percentag collis practic prevent collis adopt supplementari measur time node level tree distribut propos reflect delic charact oper robot train oper move slowli enough space obstacl train anoth oper brush quickli obstacl fact give us hint print charact machin posit identif robot identifi posit recal landscap posit camera purpos principl suffic regard camera imag posit xs ys howev memori capac finit actual compu ter henc compress camera imag slight loss inform compress admitt long precis posit identif accept area thu major problem come find suitabl compress experiment environ jut passageway interv section adjac jut door robot identifi roughli surround landscap section place use temporarili triangular survey techniqu exact measur necessari realiz former task defin follow turn camera take panorama deg scan horizont center line substitut point lumin excess chang black point white regard binari dot line imag process thu xs defin x accordingli everi x x e x x x project black point x onto x measur euclidean distanc black point x closest let summat similarli calcul exchang role x x denot number nand n defin dx x n n regard posit integ label section ys cf defin accordingli learn mode robot check exactli posit counter reset period oper robot run arbitrarili passageway within area learn relat landscap posit posit identif beyond area achiev cross plural databas anoth task automat except period reset counter name kind learn without teacher defin identif rate rel frequenc correct recal posit past trial typic converg around time time level level oftre distribut sinc identif failur reject consid trajectori pro blem aris practic use order improv identif rate compress ratio camera imag must loosen possibl depend improv hardwar futur show actual motion robot base databas obstacl avoid movement posit identif correspond case move time interv per frame sec ii actual motion robot conclus selforgan associ databas propos applic robot eyesight system machin decompos global structur unknown set local structur known learn univers inputoutput respons framework problem impli wide applic area exampl shown paper defect algorithm selforgan tree balanc well subclass structur f subject impos us widen class probabl solut abolish address rule depend directli instead establish anoth rule depend distribut function investig refer hopfield j j w tank comput neural circuit model scienc pp rumelhart e et al learn represent backpropag error natur pp hull j j hypothesi gener comput model visual word recognit ieee expert fall pp kurtzberg j featur symbol recognit elast match ibm j re develop pp wang q r c suen tree classifi heurist search global train ieee tran pattern anal mach intel pami pp brook r et al self calibr motion stereo vision mobil robot th int symp robot research pp goto stentz cmu system mobil robot navig ieee int conf robot autom pp madarasz r et al design autonom vehicl disabl ieee jour robot autom ra pp triendl e j kriegman stereo vision navig within build ieee int conf robot autom pp turk et al video roadfollow autonom land vehicl ieee int conf robot autom pp'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40c02bc5-ede2-4904-8597-f25cc199dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58d0285b-425b-4541-8b3d-3e0e96cb01f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 1 0 3 1 0]\n",
      " [0 1 1 2 0 1 0 0 0 0 1 1]\n",
      " [1 1 0 0 0 0 1 1 1 1 0 1]]\n",
      "['and' 'data' 'for' 'great' 'in' 'is' 'learning' 'love' 'machine'\n",
      " 'programming' 'python' 'science']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "    \"I love programming programming programming in Python\",\n",
    "    \"Python is great great for data science\",\n",
    "    \"I love data science and machine learning programming\"\n",
    "]\n",
    "\n",
    "# Create CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array for better readability\n",
    "print(X.toarray())\n",
    "\n",
    "# Display the feature names (words)\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954c647-a6e2-4b34-87cb-0efa9704b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''How CountVectorizer Works\n",
    "It tokenizes the text (splits it into words).\n",
    "It creates a vocabulary of all unique words in the dataset.\n",
    "It represents each document as a vector where each entry corresponds to the count of a specific word in that document.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "16013b63-5ad8-4cf3-99dd-b8d33354e29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_df=0.9: Ignores terms that appear in more than 90% of the documents, treating them as too common (like stop words).\\nmax_features=7000: Limits the vocabulary size to the 7,000 most frequently occurring words.\\nngram_range=(1,3): Extracts unigrams (single words), bigrams (two-word combinations), and trigrams (three-word combinations).\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "max_df=0.9: Ignores terms that appear in more than 90% of the documents, treating them as too common (like stop words).\n",
    "max_features=7000: Limits the vocabulary size to the 7,000 most frequently occurring words.\n",
    "ngram_range=(1,3): Extracts unigrams (single words), bigrams (two-word combinations), and trigrams (three-word combinations).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df3a404f-196f-4715-9c10-9bf47107d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv= CountVectorizer(max_df=0.9,max_features=7000,ngram_range=(1,3))\n",
    "word_count_vectors=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b286be13-e43f-4435-87f7-aeefedeef1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7241x7000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5070690 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6fe39fcb-bcaf-49df-9a8d-315c04aa3971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsmooth_idf=True: This smoothing prevents division by zero when a term is not present in any document.\\nuse_idf = True: give highest value to less accuring words (keywords)\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TfidfTransformer\n",
    "'''\n",
    "CountVectorizer extracts word counts.\n",
    "TfidfTransformer converts counts into TF-IDF values.\n",
    "High TF-IDF score → Important word in that document.\n",
    "Low TF-IDF score → Common word across documents.\n",
    "'''\n",
    "\n",
    "'''\n",
    "smooth_idf=True: This smoothing prevents division by zero when a term is not present in any document.\n",
    "use_idf = True: give highest value to less accuring words (keywords)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "763153d2-fcf1-463d-bead-c696650016ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3680e8f3-4b21-47c5-af83-bc30efeacd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer= TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer=tfidf_transformer.fit(word_count_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4adf18f-e192-4719-befe-dfb9230285ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now to get keywords\n",
    "''' Get features names\n",
    "word counts for user docs \n",
    "sorting sparse matrix conditions\n",
    "extracting top 10 keywords'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d9832e66-a1b3-48d3-bf42-471f2b3660f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83340578-68a2-499c-9d2a-6cfe68bca38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(index,docs,topNum=10):\n",
    "    #we get words count and it's importance\n",
    "    docs_words_count= tfidf_transformer.transform(cv.transform([docs[index]]))\n",
    "    #sorting sparse matrix\n",
    "    docs_words_count=docs_words_count.tocoo()\n",
    "    tuples=zip(docs_words_count.col,docs_words_count.data)\n",
    "    sorted_items= sorted(tuples,key= lambda x : (x[1],x[0]),reverse=True)\n",
    "    #getting top 10 keywords\n",
    "    sorted_items=sorted_items[:topNum]\n",
    "    score_vals=[]\n",
    "    feature_vals=[]\n",
    "    for index,score in sorted_items:\n",
    "        score_vals.append(round(score,3))\n",
    "        feature_vals.append(feature_names[index])\n",
    "    #Make a dictionary to store result\n",
    "    results= {}\n",
    "    for index in range(len(feature_vals)):\n",
    "        results[feature_vals[index]]=score_vals[index]\n",
    "    return results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "834b128a-44ad-41cd-8dc9-cf9170870127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======Title======\n",
      "Integrated Non-Factorized Variational Inference\n",
      "\n",
      "=====abstract=====\n",
      "We present a non-factorized variational method for full posterior inference in Bayesian hierarchical models,  with the goal of capturing the posterior variable dependencies via efficient and possibly parallel computation. Our approach unifies the integrated nested Laplace approximation (INLA) under the variational framework. The proposed method is applicable in more challenging scenarios than typically assumed by INLA,  such as Bayesian Lasso,  which is characterized by the non-differentiability of the $\\ell_{1}$ norm arising from independent Laplace priors. We derive an upper bound for the Kullback-Leibler divergence,  which yields a fast closed-form solution via decoupled optimization. Our method is a reliable analytic alternative to Markov chain Monte Carlo (MCMC), and it results in a tighter evidence lower bound than that of mean-field variational Bayes (VB) method.\n",
      "\n",
      "=====Keywords====\n",
      "vb 0.363\n",
      "posterior 0.248\n",
      "mcmc 0.241\n",
      "variat 0.231\n",
      "qd 0.214\n",
      "lasso 0.197\n",
      "ln 0.183\n",
      "kl diverg 0.172\n",
      "bayesian 0.168\n",
      "kl 0.162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_keywords(index,keywords,df):\n",
    "    print(\"\\n======Title======\")\n",
    "    print(df['title'][index])\n",
    "    print(\"\\n=====abstract=====\")\n",
    "    print(df['abstract'][index])\n",
    "    print('\\n=====Keywords====')\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])\n",
    "\n",
    "index=4495\n",
    "keywords =get_keywords(index,docs)\n",
    "print_keywords(index,keywords,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ebb9c3c5-4ac4-450d-b10c-0470629f85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(cv, open('count_vector.pkl', 'wb'))\n",
    "pickle.dump(tfidf_transformer, open('tfidf_transformer.pkl', 'wb'))\n",
    "pickle.dump(feature_names, open('feature_names.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08497433-0639-4a05-abd6-209bed0c5e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
